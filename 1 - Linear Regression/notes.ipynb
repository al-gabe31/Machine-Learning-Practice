{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9ec312",
   "metadata": {},
   "source": [
    "# **Linear Regression Notes**\n",
    "\n",
    "## Loss Function\n",
    "Let the loss function be defined as: $$ℒ=\\frac{1}{N}\\Sigma\\left(y-ŷ\\right)^{2}$$\n",
    "* N = number of data points\n",
    "* ŷ = prediction function\n",
    "where $$ŷ=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...$$\n",
    "$$x_{0}=1$$\n",
    "* θn = parameter to be learned\n",
    "* Xn = inputs\n",
    "\n",
    "## Parameter Update Rule\n",
    "We can define the update rule for each parameter through each iteration as such: $$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}-\\alpha∇ℒ}$$\n",
    "* we're subtracting because gradients point the direction of steepest ascent\n",
    "* we want the direction of steepest descent (hence, substraction)\n",
    "\n",
    "## Partial Derivative for each Parameter\n",
    "Partial with respect to bias term: $$\\frac{∂ℒ}{∂\\theta_{0}}=-\\frac{2}{N}\\Sigma\\left(y-ŷ\\right)$$\n",
    "\n",
    "Partial with respect to non-bias term: $$\\frac{∂ℒ}{∂\\theta_{n}}=-\\frac{2}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)$$\n",
    "\n",
    "## Rewriting Update Rule\n",
    "$$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}+\\frac{2\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)}$$\n",
    "* you can just turn 2α into α since a constant * constant = constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47520751",
   "metadata": {},
   "source": [
    "# **Including Ridge Regression (L2 Regularization)**\n",
    "\n",
    "## Loss Function\n",
    "Let the loss function be defined as: $$ℒ=\\frac{1}{N}\\Sigma\\left(y-ŷ\\right)^{2}\\color{red}{+\\lambda\\Sigma\\theta^{2}}$$\n",
    "* the part in red is the regularization penalty for L2\n",
    "* notice how the rest of the loss function is identical to the one without L2 regularization\n",
    "\n",
    "## Parameter Update Rule (w/ Ridge Regression)\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}-\\alpha∇ℒ$$\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}+\\frac{2\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)\\color{red}{-2\\alpha\\lambda\\theta_{n}}$$\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}\\textcolor{red}{-2\\alpha\\lambda\\theta_{n}}+\\frac{2\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)$$\n",
    "$$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}\\left(1\\textcolor{red}{-2\\alpha\\lambda}\\right)+\\frac{2\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)}$$\n",
    "* L2 regularization makes parameters **approach 0 but never actually equal to 0**\n",
    "* unhelpful parameters converge much closer to 0 compared to helpful parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bb32a",
   "metadata": {},
   "source": [
    "# **Including Lasso Regression (L1 Regularization)**\n",
    "\n",
    "## Loss Function\n",
    "Let the loss function be defined as: $$ℒ=\\frac{1}{N}\\Sigma\\left(y-ŷ\\right)^{2}\\color{red}{+\\lambda\\Sigma\\left|\\theta\\right|}$$\n",
    "* the part in red is the regularization penalty for L1\n",
    "\n",
    "## Parameter Update Rule (w/ Lasso Regression)\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}-\\alpha∇ℒ$$\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}+\\frac{2\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)\\color{red}{-\\alpha\\lambda\\operatorname{sign}\\left(\\theta_{n}\\right)}$$\n",
    "$$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}\\textcolor{red}{-\\alpha\\lambda\\operatorname{sign}\\left(\\theta_{n}\\right)}+\\frac{2\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)}$$\n",
    "* L1 regularization makes it **possible for some parameters to equal 0**\n",
    "    * unhelpful parameters get turned to 0\n",
    "* L1 regularization automatically includes feature selection"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
