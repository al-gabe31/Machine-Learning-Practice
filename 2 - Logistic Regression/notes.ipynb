{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a207074",
   "metadata": {},
   "source": [
    "# **Logistic Regression Notes**\n",
    "\n",
    "## Loss Function\n",
    "Let the loss function be defined as: $$ℒ=\\frac{-1}{N}\\Sigma\\left(\\textcolor{red}{y\\ln\\left(ŷ\\right)+\\left(1-y\\right)\\ln\\left(1-ŷ\\right)}\\right)$$\n",
    "* the part in red is **<font color='red'>Binary Cross Entropy</font>**\n",
    "* N = number of data points\n",
    "* ŷ is the following: \n",
    "$$ŷ=\\frac{1}{1+e^{-z}}$$\n",
    "* z is the linear model: \n",
    "$$z=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...$$\n",
    "$$x_{0}=1$$\n",
    "* θn = parameter to be learned\n",
    "* Xn = inputs\n",
    "\n",
    "## Parameter Update Rule\n",
    "We can define the update rule for each parameter through each iteration as such: $$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}-\\alpha∇ℒ}$$\n",
    "To find ∇ℒ, we'll have to use the **chain rule** $$\\frac{∂ℒ}{∂\\theta_{n}}=\\textcolor{#FA5053}{\\frac{∂ℒ}{∂ŷ}}\\cdot\\textcolor{#50C878}{\\frac{∂ŷ}{∂z}}\\cdot\\textcolor{#6395EE}{\\frac{∂z}{∂\\theta_{n}}}$$\n",
    "\n",
    "## Chain Rule\n",
    "Finding each partial derivative: \n",
    "$$\\textcolor{#6395EE}{\\frac{∂z}{∂\\theta_{n}}}=\\frac{∂}{∂\\theta_{n}}\\left(\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...\\right)=x_{n}$$\n",
    "* partial with respect to the bias term = 1\n",
    "$$\\textcolor{#50C878}{\\frac{∂ŷ}{∂z}}=\\frac{∂}{∂z}\\left(\\frac{1}{1+e^{-z}}\\right)=\\frac{e^{-z}}{\\left(1+e^{-z}\\right)^{2}}$$\n",
    "$$=\\frac{e^{-z}}{1+e^{-z}}\\cdot\\frac{1}{1+e^{-z}}$$\n",
    "$$=\\frac{\\textcolor{red}{1}+e^{-z}\\textcolor{red}{-1}}{1+e^{-z}}\\cdot\\frac{1}{1+e^{-z}}$$\n",
    "$$=\\left(\\frac{1+e^{-z}}{1+e^{-z}}-\\frac{1}{1+e^{-z}}\\right)\\left(\\frac{1}{1+e^{-z}}\\right)$$\n",
    "$$\\textcolor{#50C878}{\\frac{∂ŷ}{∂z}}=\\left(1-ŷ\\right)\\left(ŷ\\right)$$\n",
    "$$\\textcolor{#FA5053}{\\frac{∂ℒ}{∂ŷ}}=-\\frac{1}{N}\\frac{∂}{∂ŷ}\\Sigma\\left(y\\ln\\left(ŷ\\right)+\\left(1-y\\right)\\ln\\left(1-ŷ\\right)\\right)$$\n",
    "$$=-\\frac{1}{N}\\Sigma\\left(\\frac{y}{ŷ}-\\frac{1-y}{1-ŷ}\\right)$$\n",
    "\n",
    "Putting it all together...\n",
    "$$\\frac{∂ℒ}{∂\\theta_{n}}=\\textcolor{#FA5053}{\\frac{∂ℒ}{∂ŷ}}\\cdot\\textcolor{#50C878}{\\frac{∂ŷ}{∂z}}\\cdot\\textcolor{#6395EE}{\\frac{∂z}{∂\\theta_{n}}}=\\textcolor{#FA5053}{-\\frac{1}{N}\\Sigma\\left(\\frac{y}{ŷ}-\\frac{1-y}{1-ŷ}\\right)}\\textcolor{#50C878}{\\left(1-ŷ\\right)\\left(ŷ\\right)}\\textcolor{#6395EE}{\\left(x_{n}\\right)}$$\n",
    "$$=-\\frac{1}{N}\\Sigma\\left(y\\textcolor{red}{-yŷ}-ŷ\\textcolor{red}{+yŷ}\\right)\\left(x_{n}\\right)$$\n",
    "$$\\frac{∂ℒ}{∂\\theta_{n}}=-\\frac{1}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)$$\n",
    "\n",
    "## Rewriting Update Rule\n",
    "$$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}+\\frac{\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)}$$\n",
    "* update rule is almost identical with that from linear regression\n",
    "* instead of 2α, it's α\n",
    "    * to be fair, a constant times a constant is just a constant..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae130d5",
   "metadata": {},
   "source": [
    "# **Including Ridge Regression (L2 Regularization)**\n",
    "\n",
    "## Loss Function\n",
    "Let the loss function be defined as: $$ℒ=\\frac{-1}{N}\\Sigma\\left(y\\ln\\left(ŷ\\right)+\\left(1-y\\right)\\ln\\left(1-ŷ\\right)\\right)\\textcolor{red}{+\\lambda\\Sigma\\theta^{2}}$$\n",
    "* the part in red is the regularization penalty for L2\n",
    "* notice how the rest of the loss function is identical to the one without L2 regularization\n",
    "\n",
    "## Parameter Update Rule (w/ Ridge Regression)\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}-\\alpha∇ℒ$$\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}+\\frac{\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)\\color{red}{-2\\alpha\\lambda\\theta_{n}}$$\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}\\textcolor{red}{-2\\alpha\\lambda\\theta_{n}}+\\frac{\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)$$\n",
    "$$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}\\left(1\\textcolor{red}{-2\\alpha\\lambda}\\right)+\\frac{\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)}$$\n",
    "* L2 regularization makes parameters **approach 0 but never actually equal to 0**\n",
    "* unhelpful parameters converge much closer to 0 compared to helpful parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f64cb",
   "metadata": {},
   "source": [
    "# **Including Lasso Regression (L1 Regularization)**\n",
    "\n",
    "## Loss Function\n",
    "Let the loss function be defined as: $$ℒ=\\frac{-1}{N}\\Sigma\\left(y\\ln\\left(ŷ\\right)+\\left(1-y\\right)\\ln\\left(1-ŷ\\right)\\right)\\color{red}{+\\lambda\\Sigma\\left|\\theta\\right|}$$\n",
    "* the part in red is the regularization penalty for L1\n",
    "\n",
    "## Parameter Update Rule (w/ Lasso Regression)\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}-\\alpha∇ℒ$$\n",
    "$$\\theta_{n}^{t+1}=\\theta_{n}+\\frac{\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)\\color{red}{-\\alpha\\lambda\\operatorname{sign}\\left(\\theta_{n}\\right)}$$\n",
    "$$\\boxed{\\theta_{n}^{t+1}=\\theta_{n}\\textcolor{red}{-\\alpha\\lambda\\operatorname{sign}\\left(\\theta_{n}\\right)}+\\frac{\\alpha}{N}\\Sigma\\left(y-ŷ\\right)\\left(x_{n}\\right)}$$\n",
    "* L1 regularization makes it **possible for some parameters to equal 0**\n",
    "    * unhelpful parameters get turned to 0\n",
    "* L1 regularization automatically includes feature selection"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
